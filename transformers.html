<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="Machine Learning - conceptos, tipos, casos de uso y algoritmos" />
    <meta name="keywords" content="Machine Learning, Aprendizaje Supervisado, No Supervisado, Refuerzo, IA" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="stylesheet" href="../css/estilo.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css" />
    <title>FAI-AI — Machine Learning</title>
</head>

<body>
    <header>
        <div class="conjunto">
            <div class="logo">
                <a href="../index.html"><img src="../Imagenes/Logo.png" alt="logo"></a>
            </div>
            <div class="titulo">
                <h1><a href="../index.html">N-AI</a></h1>
            </div>
        </div>

        <nav>
            <ul class="lista">
                <li><a class="link link--1" href="../index.html">Principal</a></li>
                <li><a class="link link--1" href="../html/ml.html">Machine Learning</a></li>
                <li><a class="link link--1" href="../html/dl.html">Deep Learning</a></li>
            </ul>
        </nav>
    </header>
    <main id="main-dl">
        <selection id="selection-cnn">
            <h2 id="modelos-h2-cnn">Transformers</h2>
            <div id="modelos-cajas-cnn">
                <p>
                    Los modelos de transformador combinan una arquitectura de codificador-decodificador con un mecanismo de
                     procesamiento de texto y han revolucionado la forma en que se entrenan los modelos de lenguaje.
                      Un codificador convierte el texto sin procesar, sin anotar, en representaciones conocidas como
                       incrustaciones; el descodificador toma estas incrustaciones junto con los resultados anteriores 
                       del modelo y predice sucesivamente cada palabra de una frase.
                </p>
                <p>
                    Mediante adivinanzas para completar espacios en blanco, el codificador aprende cómo se relacionan entre sí
                     las palabras y las frases, construyendo una potente representación del lenguaje sin tener que etiquetar las 
                     partes de la oración y otros rasgos gramaticales. Los transformadores, de hecho, se pueden entrenar previamente
                      desde el principio sin una tarea particular en mente. Una vez que se aprenden estas poderosas representaciones,
                       los modelos se pueden especializar, con muchos menos datos, para realizar una tarea solicitada.
                </p>
                <p>
                    Varias innovaciones lo hacen posible. Los transformadores procesan las palabras de una oración simultáneamente,
                     lo que permite procesar el texto en paralelo y acelera el entrenamiento. Las técnicas anteriores, incluidas las 
                     redes neuronales recurrentes (RNN), procesaban las palabras una por una. Los transformadores también aprendieron las
                      posiciones de las palabras y sus relaciones: este contexto les permite inferir significados y desambiguar palabras
                       como "eso" en frases largas.
                </p>
                <p>
                    Al eliminar la necesidad de definir una tarea por adelantado, los transformadores hicieron que fuera práctico entrenar
                     previamente los modelos de lenguaje en grandes cantidades de texto sin formato, lo que les permitió crecer dramáticamente
                      en tamaño. Anteriormente, los datos etiquetados se recopilaban para entrenar un modelo en una tarea específica. Con los
                       transformadores, un modelo entrenado con una gran cantidad de datos se puede adaptar a múltiples tareas ajustándolo en
                        una pequeña cantidad de datos etiquetados específicos de la tarea.
                </p>
                <p>
                    Hoy en día, los transformadores de lenguaje se utilizan para tareas no generativas, como la clasificación y la extracción
                     de entidades, así como para tareas generativas, como la traducción automática, el resumen y la respuesta a preguntas.
                      Los transformadores han sorprendido a muchas personas con su capacidad para generar diálogos, ensayos y otros contenidos
                       convincentes.
                </p>
                <p>
                    Los transformadores de procesamiento del lenguaje natural (PLN) proporcionan una potencia notable, ya que pueden 
                    ejecutarse en paralelo, procesando múltiples partes de una secuencia simultáneamente, lo que acelera enormemente 
                    el entrenamiento. Los transformadores también rastrean las dependencias a largo plazo en el texto, lo que les permite
                     entender el contexto general con mayor claridad y crear resultados superiores. Además, los transformadores son más
                      escalables y flexibles para personalizarlos según las tareas.
                </p>
                <p>
                    En cuanto a las limitaciones, debido a su complejidad, los transformadores requieren enormes recursos computacionales
                     y un largo tiempo de entrenamiento. Además, los datos de entrenamiento deben ser precisos, imparciales y abundantes 
                     para producir resultados exactos.
                </p>
            </div>
        </selection>
    </main>

</body>

</html>